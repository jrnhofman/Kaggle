{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training sample:  (891, 12)\n",
      "Shape of testing sample:  (418, 11)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(\"Shape of training sample: \",train_df.shape)\n",
    "print(\"Shape of testing sample: \",test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.pairplot(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any nulls?\n",
    "train_df.isna().sum(), test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticket / Name / PassengerId seems to be ver sparse - how many uniques we have?\n",
    "print(train_df['Ticket'].nunique(), ' out of ', train_df.shape[0])\n",
    "print(train_df['PassengerId'].nunique(), ' out of ', train_df.shape[0])\n",
    "print(train_df['Name'].nunique(), ' out of ', train_df.shape[0])\n",
    "print(train_df['Cabin'].nunique(), ' out of ', train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the other categorical features\n",
    "categorical_columns = ['Survived', 'Pclass', 'Sex', 'Embarked']\n",
    "\n",
    "for c in categorical_columns:\n",
    "    print(\"Feature: \", c)\n",
    "    print(\"Train: \")\n",
    "    print(train_df[c].value_counts())\n",
    "    if c is not 'Survived':\n",
    "        print(\"Test: \")\n",
    "        print(test_df[c].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at age, the text on Kaggle is a bit ambiguous about what age<1 means\n",
    "train_df['Age'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks okay, just a few babies it seems to me\n",
    "train_df[train_df['Age']<=1]['Age'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text on Kaggle also mentions estimated ages have xx.5, how many are those?\n",
    "print(train_df[(train_df['Age']-np.floor(train_df['Age']))==0.5].shape[0], ' out of ', train_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive first approach, RF CV with very simple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that are very sparse or have a lot of nulls\n",
    "X = train_df.copy().drop('Survived',axis=1)\n",
    "y = np.array(train_df['Survived']).ravel()\n",
    "X_test = test_df.copy()\n",
    "\n",
    "cat_columns = ['Embarked','Pclass','Sex']\n",
    "cont_columns = ['Age','SibSp','Parch','Fare']\n",
    "drop_columns = [x for x in X.columns if x not in (cat_columns + cont_columns)]\n",
    "print('Dropping: ',drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(drop_columns,axis=1,inplace=True)\n",
    "X_test.drop(drop_columns,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing age with median, missing 'Embarked' with mode\n",
    "X = X.fillna({'Age' : X['Age'].median(), 'Embarked' : X['Embarked'].mode()[0]})\n",
    "X_test = X_test.fillna({'Age' : X['Age'].median(), 'Fare' : X['Fare'].median()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "[('bla',OneHotEncoder(categories='auto'),cat_columns)],remainder='passthrough')\n",
    "\n",
    "X_ohe = ct.fit_transform(X)\n",
    "X_test_ohe = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param grid\n",
    "param_grid = {\n",
    "    'n_estimators': [2,4,8,16,32,64,128],\n",
    "    'max_depth' : [200,100,50,20],\n",
    "    'criterion' : ['entropy'],\n",
    "    'min_samples_split' : [0.1,0.3,0.5,0.75,1.0],\n",
    "    'min_samples_leaf' : np.linspace(0.1,0.5,num=5),\n",
    "    'max_features' : np.linspace(0.5,1.0,num=5+1)\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "gscv = GridSearchCV(rf,param_grid=param_grid,scoring='accuracy',cv=10,n_jobs=-1,verbose=10)\n",
    "\n",
    "result = gscv.fit(X=X_ohe,y=y)\n",
    "print(result.best_params_)\n",
    "print(result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = result.best_estimator_\n",
    "best = best_rf_model.fit(X_ohe,y)\n",
    "predictions = best.predict(X_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set of predictions - not very good - 0.76555 on public leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see, mostof the featuers are ignored, we have to do some more feature engineering\n",
    "best_rf_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but it seems some of the other features are relevant but they are not included in the feature importances...\n",
    "train_df.groupby('Embarked').Survived.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other ideas to try\n",
    "# Xgboost or something similar\n",
    "# Interactions\n",
    "# Extract more information from features not used now or with 0 importance\n",
    "# Different imputations for age, Can we infer age from name?\n",
    "# Kid with parent?\n",
    "# Lived in cabin? (cabin not none)\n",
    "# cabin location?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More features & smarter imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For missing values we consider both train and test to not bias\n",
    "df_all = pd.concat([train_df,test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age            1.000000\n",
      "Fare           0.178740\n",
      "Parch         -0.150917\n",
      "PassengerId    0.028814\n",
      "Pclass        -0.408106\n",
      "SibSp         -0.243699\n",
      "Survived      -0.077221\n",
      "dtype: float64\n",
      "Average age per Pclass: \n",
      "Pclass\n",
      "1    39.0\n",
      "2    29.0\n",
      "3    24.0\n",
      "Name: Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Missing values - Age\n",
    "print(df_all.corrwith(df_all['Age']))\n",
    "\n",
    "# Age is very correlated with pclass, so imputing with class averages\n",
    "avg_age = df_all.groupby('Pclass')['Age'].median()\n",
    "print('Average age per Pclass: ')\n",
    "print(avg_age)\n",
    "\n",
    "df_all['Age'] = np.where( (df_all['Age'].isnull()) & (df_all['Pclass']==1), avg_age[1],\n",
    "                         np.where((df_all['Age'].isnull()) & (df_all['Pclass']==2), avg_age[2],\n",
    "                                 np.where((df_all['Age'].isnull()) & (df_all['Pclass']==3), avg_age[3],df_all['Age'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Cabin', 'Embarked', 'Fare', 'Name', 'Parch', 'PassengerId',\n",
       "       'Pclass', 'Sex', 'SibSp', 'Survived', 'Ticket'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values - Embarked\n",
    "# Taking simple mode as only 2 are missing\n",
    "df_all = df_all.fillna({'Embarked' : df_all['Embarked'].mode()[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Age Cabin Embarked  Fare                Name  Parch  PassengerId  \\\n",
      "152  60.5   NaN        S   NaN  Storey, Mr. Thomas      0         1044   \n",
      "\n",
      "     Pclass   Sex  SibSp  Survived Ticket  \n",
      "152       3  male      0       NaN   3701  \n"
     ]
    }
   ],
   "source": [
    "# Missing values - Fare\n",
    "print(df_all[df_all['Fare'].isnull()])\n",
    "\n",
    "# Only one, impute with average male fare with pclass 3\n",
    "df_all = df_all.fillna({'Fare': df_all[ (df_all.Sex=='male') & (df_all.Pclass==3) ]['Fare'].median()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values - Cabin\n",
    "# Very difficult to figure out - treating missing values as a separate category\n",
    "df_all['Cabin'] = df_all['Cabin'].apply(lambda x: str(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age              0\n",
       "Cabin            0\n",
       "Embarked         0\n",
       "Fare             0\n",
       "Name             0\n",
       "Parch            0\n",
       "PassengerId      0\n",
       "Pclass           0\n",
       "Sex              0\n",
       "SibSp            0\n",
       "Survived       418\n",
       "Ticket           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All good!\n",
    "df_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     790\n",
      "2     235\n",
      "3     159\n",
      "4      43\n",
      "5      22\n",
      "6      25\n",
      "7      16\n",
      "8       8\n",
      "11     11\n",
      "Name: Family_size, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_all['Family_size'] = 1 + df_all['SibSp'] + df_all['Parch']\n",
    "# Family sizes\n",
    "print(df_all['Family_size'].value_counts().sort_index())\n",
    "\n",
    "# Mapping for new feature\n",
    "family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Medium', 5: 'Medium', 6: 'Large', 7: 'Large', 8: 'Large', 11: 'Large'}\n",
    "df_all['Family_size'] = df_all['Family_size'].map(family_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Married woman\n",
    "df_all['Has_husband'] = np.where((df_all['Name'].str.contains('Mrs')) & (df_all['Sex']=='female') & (df_all['SibSp']==1),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr                          757\n",
       "Miss/Mrs/Ms                 464\n",
       "Master                       61\n",
       "Dr/Military/Noble/Clergy     27\n",
       "Name: Title, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Titles\n",
    "df_all['Title'] = df_all['Name'].str.split(',').apply(lambda x: x[1]).str.split(' ').apply(lambda x:x[1][:-1])\n",
    "#df_all['Title'].value_counts()\n",
    "\n",
    "# Map weird titles\n",
    "df_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\n",
    "df_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev', 'th'], 'Dr/Military/Noble/Clergy')\n",
    "\n",
    "df_all['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns\n",
    "\n",
    "df_train = df_all[~df_all.Survived.isnull()]\n",
    "df_test = df_all[df_all.Survived.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['Embarked','Pclass','Sex','Cabin','Family_size','Has_husband','Title']\n",
    "cont_columns = ['Age','Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoded_features = []\n",
    "\n",
    "# Some ugly code to join transformed features back\n",
    "# in readable format to train and test df\n",
    "for df in [df_train,df_test]:\n",
    "    for feature in cat_columns:\n",
    "        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n",
    "        n = df[feature].nunique()\n",
    "        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n",
    "        \n",
    "        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n",
    "        encoded_df.index = df.index\n",
    "        encoded_features.append(encoded_df)\n",
    "\n",
    "df_train = pd.concat([df_train, *encoded_features[:len(cat_columns)]], axis=1)\n",
    "df_test = pd.concat([df_test, *encoded_features[len(cat_columns):]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = (['Age','Fare', 'Embarked_1', 'Embarked_2', 'Embarked_3',\n",
    "       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_1', 'Sex_2', 'Cabin_1',\n",
    "       'Cabin_2', 'Cabin_3', 'Cabin_4', 'Cabin_5', 'Cabin_6', 'Cabin_7',\n",
    "       'Cabin_8', 'Cabin_9', 'Family_size_1', 'Family_size_2', 'Family_size_3',\n",
    "       'Family_size_4', 'Has_husband_1', 'Has_husband_2', 'Title_1', 'Title_2', 'Title_3', 'Title_4', 'Survived'])\n",
    "keep_columns.remove('Cabin_9')\n",
    "df_train = df_train[keep_columns]\n",
    "keep_columns.remove('Survived')\n",
    "df_test = df_test[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Fare', 'Embarked_1', 'Embarked_2', 'Embarked_3', 'Pclass_1',\n",
       "       'Pclass_2', 'Pclass_3', 'Sex_1', 'Sex_2', 'Cabin_1', 'Cabin_2',\n",
       "       'Cabin_3', 'Cabin_4', 'Cabin_5', 'Cabin_6', 'Cabin_7', 'Cabin_8',\n",
       "       'Family_size_1', 'Family_size_2', 'Family_size_3', 'Family_size_4',\n",
       "       'Has_husband_1', 'Has_husband_2', 'Title_1', 'Title_2', 'Title_3',\n",
       "       'Title_4', 'Survived'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Fare', 'Embarked_1', 'Embarked_2', 'Embarked_3', 'Pclass_1',\n",
       "       'Pclass_2', 'Pclass_3', 'Sex_1', 'Sex_2', 'Cabin_1', 'Cabin_2',\n",
       "       'Cabin_3', 'Cabin_4', 'Cabin_5', 'Cabin_6', 'Cabin_7', 'Cabin_8',\n",
       "       'Family_size_1', 'Family_size_2', 'Family_size_3', 'Family_size_4',\n",
       "       'Has_husband_1', 'Has_husband_2', 'Title_1', 'Title_2', 'Title_3',\n",
       "       'Title_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6300 candidates, totalling 63000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1899s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1547s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  86 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1467s.) Setting batch_size=10.\n",
      "[Parallel(n_jobs=-1)]: Done 122 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 332 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 502 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 692 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 882 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1092 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1302 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1532 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1762 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2012 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2262 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too slow (2.0373s.) Setting batch_size=5.\n",
      "[Parallel(n_jobs=-1)]: Done 2517 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done 2687 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2837 tasks      | elapsed:   19.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2982 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done 3137 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done 3292 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done 3457 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=-1)]: Done 3622 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=-1)]: Done 3797 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done 3972 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=-1)]: Done 4157 tasks      | elapsed:   29.6s\n",
      "[Parallel(n_jobs=-1)]: Done 4342 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done 4537 tasks      | elapsed:   32.7s\n",
      "[Parallel(n_jobs=-1)]: Done 4732 tasks      | elapsed:   34.4s\n",
      "[Parallel(n_jobs=-1)]: Done 4937 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=-1)]: Done 5142 tasks      | elapsed:   38.0s\n",
      "[Parallel(n_jobs=-1)]: Done 5357 tasks      | elapsed:   40.0s\n",
      "[Parallel(n_jobs=-1)]: Done 5572 tasks      | elapsed:   41.9s\n",
      "[Parallel(n_jobs=-1)]: Done 5797 tasks      | elapsed:   43.9s\n",
      "[Parallel(n_jobs=-1)]: Done 6022 tasks      | elapsed:   46.1s\n",
      "[Parallel(n_jobs=-1)]: Done 6257 tasks      | elapsed:   48.3s\n",
      "[Parallel(n_jobs=-1)]: Done 6492 tasks      | elapsed:   49.9s\n",
      "[Parallel(n_jobs=-1)]: Done 6737 tasks      | elapsed:   51.3s\n",
      "[Parallel(n_jobs=-1)]: Done 6982 tasks      | elapsed:   52.5s\n",
      "[Parallel(n_jobs=-1)]: Done 7237 tasks      | elapsed:   54.2s\n",
      "[Parallel(n_jobs=-1)]: Done 7492 tasks      | elapsed:   55.6s\n",
      "[Parallel(n_jobs=-1)]: Done 7757 tasks      | elapsed:   57.3s\n",
      "[Parallel(n_jobs=-1)]: Done 8022 tasks      | elapsed:   59.0s\n",
      "[Parallel(n_jobs=-1)]: Done 8297 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8572 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8857 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 9142 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 9437 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 9732 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10037 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10342 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 10657 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 10972 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 11297 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 11622 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 11957 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 12292 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 12637 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 12982 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 13337 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 13692 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 14057 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 14422 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 14797 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 15172 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 15557 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 15942 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 16337 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 16732 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 17137 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 17542 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 17957 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 18372 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 18797 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 19222 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 19657 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 20092 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 20537 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 20982 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 21437 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 21892 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 22357 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 22822 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 23297 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 23772 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 24257 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 24742 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 25237 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 25732 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 26237 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 26742 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 27257 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 27772 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 28297 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 28822 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 29357 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 29892 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 30437 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 30982 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 31537 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 32092 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 32657 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 33222 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 33797 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 34372 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 34957 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 35542 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 36137 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 36732 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 37337 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 37942 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 38557 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 39172 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 39797 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 40422 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 41057 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 41692 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 42337 tasks      | elapsed:  5.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 42982 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 43637 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 44292 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 44957 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 45622 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 46297 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 46972 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 47657 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 48342 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 49037 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 49732 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 50437 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 51142 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 51857 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 52572 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 53297 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 54022 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 54757 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 55492 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 56237 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 56982 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 57737 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 58492 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 59257 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 60022 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 60797 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 61572 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 62357 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 62977 out of 63000 | elapsed:  7.4min remaining:    0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 50, 'max_features': 0.8, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 16}\n",
      "0.8507295173961841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 63000 out of 63000 | elapsed:  7.4min finished\n"
     ]
    }
   ],
   "source": [
    "# Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = df_train.copy().drop('Survived',axis=1)\n",
    "y = np.array(df_train['Survived']).ravel()\n",
    "X_test = df_test.copy()\n",
    "\n",
    "# param grid\n",
    "param_grid = {\n",
    "    'n_estimators': [2,4,8,16,32,64,128],\n",
    "    'max_depth' : [200,100,50,20,10],\n",
    "    'criterion' : ['entropy','gini'],\n",
    "    'min_samples_split' : [2,4,8], #[0.1,0.3,0.5,0.75,1.0],\n",
    "    'min_samples_leaf' : [1,2,4], #np.linspace(0.1,0.5,num=5),\n",
    "    'max_features' : np.linspace(0.1,1.0,num=10)\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "gscv = GridSearchCV(rf,param_grid=param_grid,scoring='accuracy',cv=10,n_jobs=-1,verbose=10)\n",
    "\n",
    "result = gscv.fit(X=X,y=y)\n",
    "print(result.best_params_)\n",
    "print(result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = result.best_estimator_\n",
    "best = best_rf_model.fit(X,y)\n",
    "predictions = best.predict(X_test).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best score now 0.77033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18542883, 0.21393965, 0.00616225, 0.007352  , 0.00927362,\n",
       "       0.02553438, 0.00864756, 0.08194384, 0.09158007, 0.02510936,\n",
       "       0.0011424 , 0.00127282, 0.00716925, 0.00182407, 0.00428699,\n",
       "       0.        , 0.        , 0.        , 0.0105591 , 0.02767309,\n",
       "       0.0040781 , 0.00954684, 0.00129774, 0.00316272, 0.01535054,\n",
       "       0.00953438, 0.06043417, 0.18769622])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Fare', 'Embarked_1', 'Embarked_2', 'Embarked_3', 'Pclass_1',\n",
       "       'Pclass_2', 'Pclass_3', 'Sex_1', 'Sex_2', 'Cabin_1', 'Cabin_2',\n",
       "       'Cabin_3', 'Cabin_4', 'Cabin_5', 'Cabin_6', 'Cabin_7', 'Cabin_8',\n",
       "       'Family_size_1', 'Family_size_2', 'Family_size_3', 'Family_size_4',\n",
       "       'Has_husband_1', 'Has_husband_2', 'Title_1', 'Title_2', 'Title_3',\n",
       "       'Title_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fare?\n",
    "## XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"solution_3.csv\"\n",
    "message = \"RF CV with advanced features - bug fixed\"\n",
    "header = ['PassengerId','Survived']\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame(\n",
    "    data=list(zip([x for x in test_df['PassengerId'].tolist()], predictions.tolist()))\n",
    ").to_csv('{}'.format(file_name), index=False, header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to Titanic: Machine Learning from Disaster"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0.00/2.77k [00:00<?, ?B/s]\r",
      "100%|██████████| 2.77k/2.77k [00:07<00:00, 356B/s]\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$file_name\" \"$message\"\n",
    "kaggle competitions submit -c titanic -f $1 -m \"$2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
