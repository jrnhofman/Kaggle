{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training sample:  (891, 12)\n",
      "Shape of testing sample:  (418, 11)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(\"Shape of training sample: \",train_df.shape)\n",
    "print(\"Shape of testing sample: \",test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.pairplot(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any nulls?\n",
    "train_df.isna().sum(), test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticket / Name / PassengerId seems to be ver sparse - how many uniques we have?\n",
    "print(train_df['Ticket'].nunique(), ' out of ', train_df.shape[0])\n",
    "print(train_df['PassengerId'].nunique(), ' out of ', train_df.shape[0])\n",
    "print(train_df['Name'].nunique(), ' out of ', train_df.shape[0])\n",
    "print(train_df['Cabin'].nunique(), ' out of ', train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the other categorical features\n",
    "categorical_columns = ['Survived', 'Pclass', 'Sex', 'Embarked']\n",
    "\n",
    "for c in categorical_columns:\n",
    "    print(\"Feature: \", c)\n",
    "    print(\"Train: \")\n",
    "    print(train_df[c].value_counts())\n",
    "    if c is not 'Survived':\n",
    "        print(\"Test: \")\n",
    "        print(test_df[c].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at age, the text on Kaggle is a bit ambiguous about what age<1 means\n",
    "train_df['Age'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks okay, just a few babies it seems to me\n",
    "train_df[train_df['Age']<=1]['Age'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text on Kaggle also mentions estimated ages have xx.5, how many are those?\n",
    "print(train_df[(train_df['Age']-np.floor(train_df['Age']))==0.5].shape[0], ' out of ', train_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive first approach, RF CV with very simple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that are very sparse or have a lot of nulls\n",
    "X = train_df.copy().drop('Survived',axis=1)\n",
    "y = np.array(train_df['Survived']).ravel()\n",
    "X_test = test_df.copy()\n",
    "\n",
    "cat_columns = ['Embarked','Pclass','Sex']\n",
    "cont_columns = ['Age','SibSp','Parch','Fare']\n",
    "drop_columns = [x for x in X.columns if x not in (cat_columns + cont_columns)]\n",
    "print('Dropping: ',drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(drop_columns,axis=1,inplace=True)\n",
    "X_test.drop(drop_columns,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing age with median, missing 'Embarked' with mode\n",
    "X = X.fillna({'Age' : X['Age'].median(), 'Embarked' : X['Embarked'].mode()[0]})\n",
    "X_test = X_test.fillna({'Age' : X['Age'].median(), 'Fare' : X['Fare'].median()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "[('bla',OneHotEncoder(categories='auto'),cat_columns)],remainder='passthrough')\n",
    "\n",
    "X_ohe = ct.fit_transform(X)\n",
    "X_test_ohe = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param grid\n",
    "param_grid = {\n",
    "    'n_estimators': [2,4,8,16,32,64,128],\n",
    "    'max_depth' : [200,100,50,20],\n",
    "    'criterion' : ['entropy'],\n",
    "    'min_samples_split' : [0.1,0.3,0.5,0.75,1.0],\n",
    "    'min_samples_leaf' : np.linspace(0.1,0.5,num=5),\n",
    "    'max_features' : np.linspace(0.5,1.0,num=5+1)\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "gscv = GridSearchCV(rf,param_grid=param_grid,scoring='accuracy',cv=10,n_jobs=-1,verbose=10)\n",
    "\n",
    "result = gscv.fit(X=X_ohe,y=y)\n",
    "print(result.best_params_)\n",
    "print(result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = result.best_estimator_\n",
    "best = best_rf_model.fit(X_ohe,y)\n",
    "predictions = best.predict(X_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set of predictions - not very good - 0.76555 on public leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see, mostof the featuers are ignored, we have to do some more feature engineering\n",
    "best_rf_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but it seems some of the other features are relevant but they are not included in the feature importances...\n",
    "train_df.groupby('Embarked').Survived.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other ideas to try\n",
    "# Xgboost or something similar\n",
    "# Interactions\n",
    "# Extract more information from features not used now or with 0 importance\n",
    "# Different imputations for age, Can we infer age from name?\n",
    "# Kid with parent?\n",
    "# Lived in cabin? (cabin not none)\n",
    "# cabin location?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More features & smarter imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For missing values we consider both train and test to not bias\n",
    "df_all = pd.concat([train_df,test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age            1.000000\n",
      "Fare           0.178740\n",
      "Parch         -0.150917\n",
      "PassengerId    0.028814\n",
      "Pclass        -0.408106\n",
      "SibSp         -0.243699\n",
      "Survived      -0.077221\n",
      "dtype: float64\n",
      "Average age per Pclass: \n",
      "Pclass\n",
      "1    39.0\n",
      "2    29.0\n",
      "3    24.0\n",
      "Name: Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Missing values - Age\n",
    "print(df_all.corrwith(df_all['Age']))\n",
    "\n",
    "# Age is very correlated with pclass, so imputing with class averages\n",
    "avg_age = df_all.groupby('Pclass')['Age'].median()\n",
    "print('Average age per Pclass: ')\n",
    "print(avg_age)\n",
    "\n",
    "df_all['Age'] = np.where( (df_all['Age'].isnull()) & (df_all['Pclass']==1), avg_age[1],\n",
    "                         np.where((df_all['Age'].isnull()) & (df_all['Pclass']==2), avg_age[2],\n",
    "                                 np.where((df_all['Age'].isnull()) & (df_all['Pclass']==3), avg_age[3],df_all['Age'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Cabin', 'Embarked', 'Fare', 'Name', 'Parch', 'PassengerId',\n",
       "       'Pclass', 'Sex', 'SibSp', 'Survived', 'Ticket'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values - Embarked\n",
    "# Taking simple mode as only 2 are missing\n",
    "df_all = df_all.fillna({'Embarked' : df_all['Embarked'].mode()[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Age Cabin Embarked  Fare                Name  Parch  PassengerId  \\\n",
      "152  60.5   NaN        S   NaN  Storey, Mr. Thomas      0         1044   \n",
      "\n",
      "     Pclass   Sex  SibSp  Survived Ticket  \n",
      "152       3  male      0       NaN   3701  \n"
     ]
    }
   ],
   "source": [
    "# Missing values - Fare\n",
    "print(df_all[df_all['Fare'].isnull()])\n",
    "\n",
    "# Only one, impute with average male fare with pclass 3\n",
    "df_all = df_all.fillna({'Fare': df_all[ (df_all.Sex=='male') & (df_all.Pclass==3) ]['Fare'].median()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values - Cabin\n",
    "# Very difficult to figure out - treating missing values as a separate category\n",
    "df_all['Cabin'] = df_all['Cabin'].apply(lambda x: str(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age              0\n",
       "Cabin            0\n",
       "Embarked         0\n",
       "Fare             0\n",
       "Name             0\n",
       "Parch            0\n",
       "PassengerId      0\n",
       "Pclass           0\n",
       "Sex              0\n",
       "SibSp            0\n",
       "Survived       418\n",
       "Ticket           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All good!\n",
    "df_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     790\n",
      "2     235\n",
      "3     159\n",
      "4      43\n",
      "5      22\n",
      "6      25\n",
      "7      16\n",
      "8       8\n",
      "11     11\n",
      "Name: Family_size, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_all['Family_size'] = 1 + df_all['SibSp'] + df_all['Parch']\n",
    "# Family sizes\n",
    "print(df_all['Family_size'].value_counts().sort_index())\n",
    "\n",
    "# Mapping for new feature\n",
    "family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Medium', 5: 'Medium', 6: 'Large', 7: 'Large', 8: 'Large', 11: 'Large'}\n",
    "df_all['Family_size'] = df_all['Family_size'].map(family_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Married woman\n",
    "df_all['Has_husband'] = np.where((df_all['Name'].str.contains('Mrs')) & (df_all['Sex']=='female') & (df_all['SibSp']==1),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr                          757\n",
       "Miss/Mrs/Ms                 464\n",
       "Master                       61\n",
       "Dr/Military/Noble/Clergy     27\n",
       "Name: Title, dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Titles\n",
    "df_all['Title'] = df_all['Name'].str.split(',').apply(lambda x: x[1]).str.split(' ').apply(lambda x:x[1][:-1])\n",
    "#df_all['Title'].value_counts()\n",
    "\n",
    "# Map weird titles\n",
    "df_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\n",
    "df_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev', 'th'], 'Dr/Military/Noble/Clergy')\n",
    "\n",
    "df_all['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns\n",
    "\n",
    "df_train = df_all[~df_all.Survived.isnull()]\n",
    "df_test = df_all[df_all.Survived.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['Embarked','Pclass','Sex','Cabin','Family_size','Has_husband','Title']\n",
    "cont_columns = ['Age','Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoded_features = []\n",
    "\n",
    "# Some ugly code to join transformed features back\n",
    "# in readable format to train and test df\n",
    "for df in [df_train,df_test]:\n",
    "    for feature in cat_columns:\n",
    "        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n",
    "        n = df[feature].nunique()\n",
    "        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n",
    "        \n",
    "        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n",
    "        encoded_df.index = df.index\n",
    "        encoded_features.append(encoded_df)\n",
    "\n",
    "df_train = pd.concat([df_train, *encoded_features[:len(cat_columns)]], axis=1)\n",
    "df_test = pd.concat([df_test, *encoded_features[len(cat_columns):]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = (['Age','Fare','Title', 'Embarked_1', 'Embarked_2', 'Embarked_3',\n",
    "       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_1', 'Sex_2', 'Cabin_1',\n",
    "       'Cabin_2', 'Cabin_3', 'Cabin_4', 'Cabin_5', 'Cabin_6', 'Cabin_7',\n",
    "       'Cabin_8', 'Cabin_9', 'Family_size_1', 'Family_size_2', 'Family_size_3',\n",
    "       'Family_size_4', 'Has_husband_1', 'Has_husband_2', 'Title_1', 'Title_2', 'Title_3', 'Title_4', 'Survived'])\n",
    "keep_columns.remove('Cabin_9')\n",
    "df_train = df_train[keep_columns]\n",
    "keep_columns.remove('Survived')\n",
    "df_test = df_test[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Fare', 'Title', 'Embarked_1', 'Embarked_2', 'Embarked_3',\n",
       "       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_1', 'Sex_2', 'Cabin_1',\n",
       "       'Cabin_2', 'Cabin_3', 'Cabin_4', 'Cabin_5', 'Cabin_6', 'Cabin_7',\n",
       "       'Cabin_8', 'Family_size_1', 'Family_size_2', 'Family_size_3',\n",
       "       'Family_size_4', 'Has_husband_1', 'Has_husband_2', 'Title_1', 'Title_2',\n",
       "       'Title_3', 'Title_4', 'Survived'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Fare', 'Title', 'Embarked_1', 'Embarked_2', 'Embarked_3',\n",
       "       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_1', 'Sex_2', 'Cabin_1',\n",
       "       'Cabin_2', 'Cabin_3', 'Cabin_4', 'Cabin_5', 'Cabin_6', 'Cabin_7',\n",
       "       'Cabin_8', 'Family_size_1', 'Family_size_2', 'Family_size_3',\n",
       "       'Family_size_4', 'Has_husband_1', 'Has_husband_2', 'Title_1', 'Title_2',\n",
       "       'Title_3', 'Title_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 168 candidates, totalling 1680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1850s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1697s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  63 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 135 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 195 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 263 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 331 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 407 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 483 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 567 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 651 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 743 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 835 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 935 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1035 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1143 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1251 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1367 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1483 tasks      | elapsed:   12.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 100, 'max_features': 0.6, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 8}\n",
      "0.8271604938271605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1680 out of 1680 | elapsed:   15.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = df_train.copy().drop('Survived',axis=1)\n",
    "y = np.array(df_train['Survived']).ravel()\n",
    "X_test = df_test.copy()\n",
    "\n",
    "# param grid\n",
    "param_grid = {\n",
    "    'n_estimators': [2,4,8,16,32,64,128],\n",
    "    'max_depth' : [200,100,50,20],\n",
    "    'criterion' : ['entropy'],\n",
    "    'min_samples_split' : [2], #[0.1,0.3,0.5,0.75,1.0],\n",
    "    'min_samples_leaf' : [1], #np.linspace(0.1,0.5,num=5),\n",
    "    'max_features' : np.linspace(0.5,1.0,num=5+1)\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "gscv = GridSearchCV(rf,param_grid=param_grid,scoring='accuracy',cv=10,n_jobs=-1,verbose=10)\n",
    "\n",
    "result = gscv.fit(X=X_ohe,y=y)\n",
    "print(result.best_params_)\n",
    "print(result.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"solution_2.csv\"\n",
    "message = \"RF CV with advanced features\"\n",
    "header = ['PassengerId','Survived']\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame(\n",
    "    data=list(zip([x for x in test_df['PassengerId'].tolist()], predictions.tolist()))\n",
    ").to_csv('{}'.format(file_name), index=False, header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-07 20:07:30,171 WARNING Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f52dc26af50>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /api/v1/competitions/titanic/submissions/url/2839/1557252449\n",
      "2019-05-07 20:07:30,171 WARNING Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f52dc282150>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /api/v1/competitions/titanic/submissions/url/2839/1557252449\n",
      "2019-05-07 20:07:30,172 WARNING Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f52dc2822d0>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /api/v1/competitions/titanic/submissions/url/2839/1557252449\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jhofman/.local/bin/kaggle\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/kaggle/cli.py\", line 51, in main\n",
      "    out = args.func(**command_args)\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/kaggle/api/kaggle_api_extended.py\", line 524, in competition_submit_cli\n",
      "    competition, quiet)\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/kaggle/api/kaggle_api_extended.py\", line 476, in competition_submit\n",
      "    last_modified_date_utc=int(os.path.getmtime(file_name))))\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/kaggle/api/kaggle_api.py\", line 971, in competitions_submissions_url_with_http_info\n",
      "    collection_formats=collection_formats)\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/kaggle/api_client.py\", line 334, in call_api\n",
      "    _preload_content, _request_timeout)\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/kaggle/api_client.py\", line 165, in __call_api\n",
      "    _request_timeout=_request_timeout)\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/kaggle/api_client.py\", line 377, in request\n",
      "    body=body)\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/kaggle/rest.py\", line 288, in POST\n",
      "    body=body)\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/kaggle/rest.py\", line 200, in request\n",
      "    headers=headers)\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/urllib3/request.py\", line 72, in request\n",
      "    **urlopen_kw)\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/urllib3/request.py\", line 150, in request_encode_body\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/urllib3/poolmanager.py\", line 323, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/urllib3/connectionpool.py\", line 638, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"/home/jhofman/.local/lib/python2.7/site-packages/urllib3/util/retry.py\", line 398, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.kaggle.com', port=443): Max retries exceeded with url: /api/v1/competitions/titanic/submissions/url/2839/1557252449 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f52dc282410>: Failed to establish a new connection: [Errno 101] Network is unreachable',))\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$file_name\" \"$message\"\n",
    "kaggle competitions submit -c titanic -f $1 -m \"$2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
