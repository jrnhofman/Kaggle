{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training sample:  (891, 12)\n",
      "Shape of testing sample:  (418, 11)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(\"Shape of training sample: \",train_df.shape)\n",
    "print(\"Shape of testing sample: \",test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.pairplot(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any nulls?\n",
    "train_df.isna().sum(), test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticket / Name / PassengerId seems to be ver sparse - how many uniques we have?\n",
    "print(train_df['Ticket'].nunique(), ' out of ', train_df.shape[0])\n",
    "print(train_df['PassengerId'].nunique(), ' out of ', train_df.shape[0])\n",
    "print(train_df['Name'].nunique(), ' out of ', train_df.shape[0])\n",
    "print(train_df['Cabin'].nunique(), ' out of ', train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the other categorical features\n",
    "categorical_columns = ['Survived', 'Pclass', 'Sex', 'Embarked']\n",
    "\n",
    "for c in categorical_columns:\n",
    "    print(\"Feature: \", c)\n",
    "    print(\"Train: \")\n",
    "    print(train_df[c].value_counts())\n",
    "    if c is not 'Survived':\n",
    "        print(\"Test: \")\n",
    "        print(test_df[c].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at age, the text on Kaggle is a bit ambiguous about what age<1 means\n",
    "train_df['Age'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks okay, just a few babies it seems to me\n",
    "train_df[train_df['Age']<=1]['Age'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text on Kaggle also mentions estimated ages have xx.5, how many are those?\n",
    "print(train_df[(train_df['Age']-np.floor(train_df['Age']))==0.5].shape[0], ' out of ', train_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive first approach, RF CV with very simple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that are very sparse or have a lot of nulls\n",
    "X = train_df.copy().drop('Survived',axis=1)\n",
    "y = np.array(train_df['Survived']).ravel()\n",
    "X_test = test_df.copy()\n",
    "\n",
    "cat_columns = ['Embarked','Pclass','Sex']\n",
    "cont_columns = ['Age','SibSp','Parch','Fare']\n",
    "drop_columns = [x for x in X.columns if x not in (cat_columns + cont_columns)]\n",
    "print('Dropping: ',drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(drop_columns,axis=1,inplace=True)\n",
    "X_test.drop(drop_columns,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing age with median, missing 'Embarked' with mode\n",
    "X = X.fillna({'Age' : X['Age'].median(), 'Embarked' : X['Embarked'].mode()[0]})\n",
    "X_test = X_test.fillna({'Age' : X['Age'].median(), 'Fare' : X['Fare'].median()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "[('bla',OneHotEncoder(categories='auto'),cat_columns)],remainder='passthrough')\n",
    "\n",
    "X_ohe = ct.fit_transform(X)\n",
    "X_test_ohe = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param grid\n",
    "param_grid = {\n",
    "    'n_estimators': [2,4,8,16,32,64,128],\n",
    "    'max_depth' : [200,100,50,20],\n",
    "    'criterion' : ['entropy'],\n",
    "    'min_samples_split' : [0.1,0.3,0.5,0.75,1.0],\n",
    "    'min_samples_leaf' : np.linspace(0.1,0.5,num=5),\n",
    "    'max_features' : np.linspace(0.5,1.0,num=5+1)\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "gscv = GridSearchCV(rf,param_grid=param_grid,scoring='accuracy',cv=10,n_jobs=-1,verbose=10)\n",
    "\n",
    "result = gscv.fit(X=X_ohe,y=y)\n",
    "print(result.best_params_)\n",
    "print(result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = result.best_estimator_\n",
    "best = best_rf_model.fit(X_ohe,y)\n",
    "predictions = best.predict(X_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set of predictions - not very good - 0.76555 on public leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see, mostof the featuers are ignored, we have to do some more feature engineering\n",
    "best_rf_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but it seems some of the other features are relevant but they are not included in the feature importances...\n",
    "train_df.groupby('Embarked').Survived.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other ideas to try\n",
    "# Xgboost or something similar\n",
    "# Interactions\n",
    "# Extract more information from features not used now or with 0 importance\n",
    "# Different imputations for age, Can we infer age from name?\n",
    "# Kid with parent?\n",
    "# Lived in cabin? (cabin not none)\n",
    "# cabin location?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More features & smarter imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For missing values we consider both train and test to not bias\n",
    "df_all = pd.concat([train_df,test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age            1.000000\n",
      "Fare           0.178740\n",
      "Parch         -0.150917\n",
      "PassengerId    0.028814\n",
      "Pclass        -0.408106\n",
      "SibSp         -0.243699\n",
      "Survived      -0.077221\n",
      "dtype: float64\n",
      "Average age per Pclass: \n",
      "Pclass\n",
      "1    39.0\n",
      "2    29.0\n",
      "3    24.0\n",
      "Name: Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Missing values - Age\n",
    "print(df_all.corrwith(df_all['Age']))\n",
    "\n",
    "# Age is very correlated with pclass, so imputing with class averages\n",
    "avg_age = df_all.groupby('Pclass')['Age'].median()\n",
    "print('Average age per Pclass: ')\n",
    "print(avg_age)\n",
    "\n",
    "df_all['Age'] = np.where( (df_all['Age'].isnull()) & (df_all['Pclass']==1), avg_age[1],\n",
    "                         np.where((df_all['Age'].isnull()) & (df_all['Pclass']==2), avg_age[2],\n",
    "                                 np.where((df_all['Age'].isnull()) & (df_all['Pclass']==3), avg_age[3],df_all['Age'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Cabin', 'Embarked', 'Fare', 'Name', 'Parch', 'PassengerId',\n",
       "       'Pclass', 'Sex', 'SibSp', 'Survived', 'Ticket'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values - Embarked\n",
    "# Taking simple mode as only 2 are missing\n",
    "df_all = df_all.fillna({'Embarked' : df_all['Embarked'].mode()[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Age Cabin Embarked  Fare                Name  Parch  PassengerId  \\\n",
      "152  60.5   NaN        S   NaN  Storey, Mr. Thomas      0         1044   \n",
      "\n",
      "     Pclass   Sex  SibSp  Survived Ticket  \n",
      "152       3  male      0       NaN   3701  \n"
     ]
    }
   ],
   "source": [
    "# Missing values - Fare\n",
    "print(df_all[df_all['Fare'].isnull()])\n",
    "\n",
    "# Only one, impute with average male fare with pclass 3\n",
    "df_all = df_all.fillna({'Fare': df_all[ (df_all.Sex=='male') & (df_all.Pclass==3) ]['Fare'].median()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values - Cabin\n",
    "# Very difficult to figure out - treating missing values as a separate category\n",
    "df_all['Cabin'] = df_all['Cabin'].apply(lambda x: str(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age              0\n",
       "Cabin            0\n",
       "Embarked         0\n",
       "Fare             0\n",
       "Name             0\n",
       "Parch            0\n",
       "PassengerId      0\n",
       "Pclass           0\n",
       "Sex              0\n",
       "SibSp            0\n",
       "Survived       418\n",
       "Ticket           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All good!\n",
    "df_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     790\n",
      "2     235\n",
      "3     159\n",
      "4      43\n",
      "5      22\n",
      "6      25\n",
      "7      16\n",
      "8       8\n",
      "11     11\n",
      "Name: Family_size, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_all['Family_size'] = 1 + df_all['SibSp'] + df_all['Parch']\n",
    "# Family sizes\n",
    "print(df_all['Family_size'].value_counts().sort_index())\n",
    "\n",
    "# Mapping for new feature\n",
    "family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Medium', 5: 'Medium', 6: 'Large', 7: 'Large', 8: 'Large', 11: 'Large'}\n",
    "df_all['Family_size'] = df_all['Family_size'].map(family_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Married woman\n",
    "df_all['Has_husband'] = np.where((df_all['Name'].str.contains('Mrs')) & (df_all['Sex']=='female') & (df_all['SibSp']==1),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr                          757\n",
       "Miss/Mrs/Ms                 464\n",
       "Master                       61\n",
       "Dr/Military/Noble/Clergy     27\n",
       "Name: Title, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Titles\n",
    "df_all['Title'] = df_all['Name'].str.split(',').apply(lambda x: x[1]).str.split(' ').apply(lambda x:x[1][:-1])\n",
    "#df_all['Title'].value_counts()\n",
    "\n",
    "# Map weird titles\n",
    "df_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\n",
    "df_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev', 'th'], 'Dr/Military/Noble/Clergy')\n",
    "\n",
    "df_all['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(x):\n",
    "    if x<80:\n",
    "        return int(np.floor(x/10))\n",
    "    if x>=80:\n",
    "        return 8\n",
    "df_all['Fare'] = df_all['Fare'].apply(categorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTicket(ticket):\n",
    "    ticket = ticket.replace('.', '')\n",
    "    ticket = ticket.replace('/', '')\n",
    "    ticket = ticket.split()\n",
    "    ticket = map(lambda t : t.strip(), ticket)\n",
    "    ticket = list(filter(lambda t : not t.isdigit(), ticket))\n",
    "    if len(ticket) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "df_all['Ticket'] = df_all['Ticket'].apply(cleanTicket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns\n",
    "\n",
    "df_train = df_all[~df_all.Survived.isnull()]\n",
    "df_test = df_all[df_all.Survived.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['Embarked','Pclass','Sex','Cabin','Family_size','Has_husband','Title','Ticket']\n",
    "cont_columns = ['Age','Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoded_features = []\n",
    "\n",
    "# Some ugly code to join transformed features back\n",
    "# in readable format to train and test df\n",
    "for df in [df_train,df_test]:\n",
    "    for feature in cat_columns:\n",
    "        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n",
    "        n = df[feature].nunique()\n",
    "        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n",
    "        \n",
    "        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n",
    "        encoded_df.index = df.index\n",
    "        encoded_features.append(encoded_df)\n",
    "\n",
    "df_train = pd.concat([df_train, *encoded_features[:len(cat_columns)]], axis=1)\n",
    "df_test = pd.concat([df_test, *encoded_features[len(cat_columns):]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Cabin', 'Embarked', 'Fare', 'Name', 'Parch', 'PassengerId',\n",
       "       'Pclass', 'Sex', 'SibSp', 'Survived', 'Ticket', 'Family_size',\n",
       "       'Has_husband', 'Title', 'Embarked_1', 'Embarked_2', 'Embarked_3',\n",
       "       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_1', 'Sex_2', 'Cabin_1',\n",
       "       'Cabin_2', 'Cabin_3', 'Cabin_4', 'Cabin_5', 'Cabin_6', 'Cabin_7',\n",
       "       'Cabin_8', 'Cabin_9', 'Family_size_1', 'Family_size_2', 'Family_size_3',\n",
       "       'Family_size_4', 'Has_husband_1', 'Has_husband_2', 'Title_1', 'Title_2',\n",
       "       'Title_3', 'Title_4', 'Ticket_1', 'Ticket_2', 'Ticket_3', 'Ticket_4',\n",
       "       'Ticket_5', 'Ticket_6', 'Ticket_7', 'Ticket_8', 'Ticket_9', 'Ticket_10',\n",
       "       'Ticket_11', 'Ticket_12', 'Ticket_13', 'Ticket_14', 'Ticket_15',\n",
       "       'Ticket_16', 'Ticket_17', 'Ticket_18', 'Ticket_19', 'Ticket_20',\n",
       "       'Ticket_21', 'Ticket_22', 'Ticket_23', 'Ticket_24', 'Ticket_25',\n",
       "       'Ticket_26', 'Ticket_27', 'Ticket_28', 'Ticket_29', 'Ticket_30',\n",
       "       'Ticket_31'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = (['Age','Fare', 'Embarked_1', 'Embarked_2', 'Embarked_3',\n",
    "       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_1', 'Sex_2', 'Cabin_1',\n",
    "       'Cabin_2', 'Cabin_3', 'Cabin_4', 'Cabin_5', 'Cabin_6', 'Cabin_7',\n",
    "       'Cabin_8', 'Cabin_9', 'Family_size_1', 'Family_size_2', 'Family_size_3',\n",
    "       'Family_size_4', 'Has_husband_1', 'Has_husband_2', 'Title_1', 'Title_2', 'Title_3', 'Title_4', 'Survived','Ticket_1', 'Ticket_2', 'Ticket_3', 'Ticket_4',\n",
    "       'Ticket_5', 'Ticket_6', 'Ticket_7', 'Ticket_8', 'Ticket_9', 'Ticket_10',\n",
    "       'Ticket_11', 'Ticket_12', 'Ticket_13', 'Ticket_14', 'Ticket_15',\n",
    "       'Ticket_16', 'Ticket_17', 'Ticket_18', 'Ticket_19', 'Ticket_20',\n",
    "       'Ticket_21', 'Ticket_22', 'Ticket_23', 'Ticket_24', 'Ticket_25',\n",
    "       'Ticket_26', 'Ticket_27', 'Ticket_28', 'Ticket_29', 'Ticket_30',\n",
    "       'Ticket_31'])\n",
    "keep_columns.remove('Cabin_9')\n",
    "keep_columns = keep_columns[:-3]\n",
    "df_train = df_train[keep_columns]\n",
    "keep_columns.remove('Survived')\n",
    "df_test = df_test[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Fare', 'Embarked_1', 'Embarked_2', 'Embarked_3', 'Pclass_1',\n",
       "       'Pclass_2', 'Pclass_3', 'Sex_1', 'Sex_2', 'Cabin_1', 'Cabin_2',\n",
       "       'Cabin_3', 'Cabin_4', 'Cabin_5', 'Cabin_6', 'Cabin_7', 'Cabin_8',\n",
       "       'Family_size_1', 'Family_size_2', 'Family_size_3', 'Family_size_4',\n",
       "       'Has_husband_1', 'Has_husband_2', 'Title_1', 'Title_2', 'Title_3',\n",
       "       'Title_4', 'Survived', 'Ticket_1', 'Ticket_2', 'Ticket_3', 'Ticket_4',\n",
       "       'Ticket_5', 'Ticket_6', 'Ticket_7', 'Ticket_8', 'Ticket_9', 'Ticket_10',\n",
       "       'Ticket_11', 'Ticket_12', 'Ticket_13', 'Ticket_14', 'Ticket_15',\n",
       "       'Ticket_16', 'Ticket_17', 'Ticket_18', 'Ticket_19', 'Ticket_20',\n",
       "       'Ticket_21', 'Ticket_22', 'Ticket_23', 'Ticket_24', 'Ticket_25',\n",
       "       'Ticket_26', 'Ticket_27', 'Ticket_28'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Fare', 'Embarked_1', 'Embarked_2', 'Embarked_3', 'Pclass_1',\n",
       "       'Pclass_2', 'Pclass_3', 'Sex_1', 'Sex_2', 'Cabin_1', 'Cabin_2',\n",
       "       'Cabin_3', 'Cabin_4', 'Cabin_5', 'Cabin_6', 'Cabin_7', 'Cabin_8',\n",
       "       'Family_size_1', 'Family_size_2', 'Family_size_3', 'Family_size_4',\n",
       "       'Has_husband_1', 'Has_husband_2', 'Title_1', 'Title_2', 'Title_3',\n",
       "       'Title_4', 'Ticket_1', 'Ticket_2', 'Ticket_3', 'Ticket_4', 'Ticket_5',\n",
       "       'Ticket_6', 'Ticket_7', 'Ticket_8', 'Ticket_9', 'Ticket_10',\n",
       "       'Ticket_11', 'Ticket_12', 'Ticket_13', 'Ticket_14', 'Ticket_15',\n",
       "       'Ticket_16', 'Ticket_17', 'Ticket_18', 'Ticket_19', 'Ticket_20',\n",
       "       'Ticket_21', 'Ticket_22', 'Ticket_23', 'Ticket_24', 'Ticket_25',\n",
       "       'Ticket_26', 'Ticket_27', 'Ticket_28'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 400 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1000s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too slow (2.0226s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too slow (2.7079s.) Setting batch_size=1.\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 187 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done 203 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done 219 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 236 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 253 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 291 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done 312 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done 356 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done 379 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done 404 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done 429 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=-1)]: Done 456 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done 483 tasks      | elapsed:   24.3s\n",
      "[Parallel(n_jobs=-1)]: Done 512 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done 541 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=-1)]: Done 572 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=-1)]: Done 603 tasks      | elapsed:   31.8s\n",
      "[Parallel(n_jobs=-1)]: Done 636 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done 669 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done 704 tasks      | elapsed:   38.1s\n",
      "[Parallel(n_jobs=-1)]: Done 739 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   43.3s\n",
      "[Parallel(n_jobs=-1)]: Done 813 tasks      | elapsed:   45.7s\n",
      "[Parallel(n_jobs=-1)]: Done 852 tasks      | elapsed:   47.9s\n",
      "[Parallel(n_jobs=-1)]: Done 891 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=-1)]: Done 932 tasks      | elapsed:   53.0s\n",
      "[Parallel(n_jobs=-1)]: Done 973 tasks      | elapsed:   56.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1016 tasks      | elapsed:   59.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1059 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1104 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1149 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1196 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1243 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1292 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1392 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1443 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1496 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1549 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1604 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1659 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1716 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1773 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1832 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1891 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1952 tasks      | elapsed:  1.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 5, 'max_features': 0.30000000000000004, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "0.8338945005611672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 2000 out of 2000 | elapsed:  1.9min finished\n"
     ]
    }
   ],
   "source": [
    "# Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = df_train.copy().drop('Survived',axis=1)\n",
    "y = np.array(df_train['Survived']).ravel()\n",
    "X_test = df_test.copy()\n",
    "\n",
    "# param grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,500,750],#[2,4,8,16,32,64,128],\n",
    "    'max_depth' : [2,3,4,5,6],#,8,10],#[20,10,5], #[200,100,50,20,10],\n",
    "    'criterion' : ['entropy','gini'],\n",
    "    'min_samples_split' : [2],#,4,8,16], #[0.1,0.3,0.5,0.75,1.0],\n",
    "    'min_samples_leaf' : [1],#,2,4,8], #np.linspace(0.1,0.5,num=5),\n",
    "    'max_features' : np.linspace(0.1,1.0,num=10)\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "gscv = GridSearchCV(rf,param_grid=param_grid,scoring='accuracy',cv=5,n_jobs=-1,verbose=10)\n",
    "\n",
    "result = gscv.fit(X=X,y=y)\n",
    "print(result.best_params_)\n",
    "print(result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = result.best_estimator_\n",
    "best = best_rf_model.fit(X,y)\n",
    "predictions = best.predict(X_test).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.06055379e-02, 5.23666916e-02, 6.57387074e-03, 4.65147739e-03,\n",
       "       1.13313316e-02, 5.35119319e-02, 1.53104366e-02, 8.68994160e-02,\n",
       "       1.91817036e-01, 1.27681542e-01, 1.13697707e-03, 3.43862089e-03,\n",
       "       3.76583452e-03, 3.25295558e-03, 1.11727767e-02, 9.26726877e-04,\n",
       "       1.17505210e-03, 1.65108101e-04, 8.57882137e-03, 3.01879591e-02,\n",
       "       4.45967196e-03, 1.67320545e-02, 4.09122893e-03, 8.01329866e-03,\n",
       "       5.73487182e-03, 1.99453794e-02, 1.18318449e-01, 1.23466577e-01,\n",
       "       8.38615735e-06, 1.93835730e-03, 0.00000000e+00, 4.93389815e-04,\n",
       "       1.75396643e-03, 0.00000000e+00, 1.82576033e-04, 1.52613704e-07,\n",
       "       0.00000000e+00, 7.55518615e-04, 2.88373377e-03, 3.05092047e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.15818977e-06,\n",
       "       0.00000000e+00, 3.34594955e-04, 5.79349044e-05, 1.04894878e-04,\n",
       "       6.16507881e-05, 2.03045129e-03, 2.92373809e-06, 2.76970288e-04,\n",
       "       0.00000000e+00, 6.75097490e-03, 9.44639387e-04, 5.80099931e-03])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Fare', 'Embarked_1', 'Embarked_2', 'Embarked_3', 'Pclass_1',\n",
       "       'Pclass_2', 'Pclass_3', 'Sex_1', 'Sex_2', 'Cabin_1', 'Cabin_2',\n",
       "       'Cabin_3', 'Cabin_4', 'Cabin_5', 'Cabin_6', 'Cabin_7', 'Cabin_8',\n",
       "       'Family_size_1', 'Family_size_2', 'Family_size_3', 'Family_size_4',\n",
       "       'Has_husband_1', 'Has_husband_2', 'Title_1', 'Title_2', 'Title_3',\n",
       "       'Title_4', 'Ticket_1', 'Ticket_2', 'Ticket_3', 'Ticket_4', 'Ticket_5',\n",
       "       'Ticket_6', 'Ticket_7', 'Ticket_8', 'Ticket_9', 'Ticket_10',\n",
       "       'Ticket_11', 'Ticket_12', 'Ticket_13', 'Ticket_14', 'Ticket_15',\n",
       "       'Ticket_16', 'Ticket_17', 'Ticket_18', 'Ticket_19', 'Ticket_20',\n",
       "       'Ticket_21', 'Ticket_22', 'Ticket_23', 'Ticket_24', 'Ticket_25',\n",
       "       'Ticket_26', 'Ticket_27', 'Ticket_28'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost (just trying, not optimizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"solution_6.csv\"\n",
    "message = \"RF - added ticket text munging\"\n",
    "header = ['PassengerId','Survived']\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame(\n",
    "    data=list(zip([x for x in test_df['PassengerId'].tolist()], [int(x) for x in predictions.tolist()]))\n",
    ").to_csv('{}'.format(file_name), index=False, header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to Titanic: Machine Learning from Disaster"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0.00/2.77k [00:00<?, ?B/s]\r",
      "100%|██████████| 2.77k/2.77k [00:00<00:00, 13.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$file_name\" \"$message\"\n",
    "kaggle competitions submit -c titanic -f $1 -m \"$2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
